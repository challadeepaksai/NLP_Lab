{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc2408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Corpus Statistics =====\n",
      "Total Sentences: 76758409\n",
      "Total Words: 767674423\n",
      "Total Characters: 4943970359\n",
      "Average Sentence Length (words): 10.00\n",
      "Average Word Length (chars): 6.44\n",
      "Type/Token Ratio (TTR): 0.0103\n"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "from collections import Counter\n",
    "\n",
    "COMMON_TLDS = {\n",
    "    'com', 'org', 'net', 'in', 'edu', 'gov', 'mil', 'info', 'biz', 'io', 'co',\n",
    "    'ai', 'me', 'app', 'dev', 'xyz', 'us', 'uk', 'ca', 'de', 'jp', 'fr', 'au',\n",
    "    'cn', 'ru', 'br', 'za', 'tv', 'cc', 'int', 'jobs', 'name', 'site',\n",
    "    'tech', 'store', 'online', 'media', 'news', 'pro', 'live', 'cloud', 'blog',\n",
    "    'club', 'solutions', 'services', 'today', 'tools'\n",
    "}\n",
    "\n",
    "def get_url_pattern(tlds):\n",
    "    tld_group = '|'.join(tlds)\n",
    "    return rf'(https?:\\/\\/)?(www\\.)?[\\w\\-]+\\.(?:{tld_group})'\n",
    "\n",
    "def get_email_pattern(tlds):\n",
    "    tld_group = '|'.join(tlds)\n",
    "    return rf'\\b[\\w\\.-]+@[\\w\\.-]+\\.(?:{tld_group})\\b'\n",
    "URL_PATTERN = get_url_pattern(COMMON_TLDS)\n",
    "EMAIL_PATTERN = get_email_pattern(COMMON_TLDS)\n",
    "DATE_PATTERN = r'\\b\\d{1,4}[-/]\\d{1,2}[-/]\\d{1,4}\\b'\n",
    "NUMBER_PATTERN = r'\\b\\d+(\\.\\d+)?\\b'\n",
    "TELUGU_PATTERN = r'[\\u0C00-\\u0C7F]+'\n",
    "ENGLISH_PATTERN = r'[a-zA-Z]+'\n",
    "PUNCTUATION_PATTERN = r'[.,!?;:\"()\\[\\]{}<>]'\n",
    "\n",
    "TOKEN_PATTERN = f\"({URL_PATTERN}|{EMAIL_PATTERN}|{DATE_PATTERN}|{NUMBER_PATTERN}|{TELUGU_PATTERN}|{ENGLISH_PATTERN}|{PUNCTUATION_PATTERN})\"\n",
    "token_regex = regex.compile(TOKEN_PATTERN)\n",
    "\n",
    "sentence_count = 0\n",
    "word_count = 0\n",
    "char_count = 0\n",
    "unique_tokens = set()\n",
    "\n",
    "def split_sentences(text):\n",
    "    safe = text.replace(\"...\", \"<ELLIPSIS>\")\n",
    "    parts = regex.split(r'(?<=[.!?])\\s+', safe)\n",
    "    return [p.replace(\"<ELLIPSIS>\", \"...\") for p in parts]\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    tokens = token_regex.findall(sentence)\n",
    "    tokens = [next(filter(None, group)) for group in tokens]\n",
    "    return tokens\n",
    "\n",
    "def clean_tokens(tokens):\n",
    "    return [t for t in tokens if not regex.fullmatch(PUNCTUATION_PATTERN, t)]\n",
    "\n",
    "input_path = \"indiccorp_telugu.txt\"\n",
    "output_path = \"tokenized_output.txt\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, \\\n",
    "     open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "\n",
    "    for line in infile:\n",
    "        paragraph = line.rstrip('\\n') \n",
    "\n",
    "        if not paragraph.strip():\n",
    "            outfile.write('\\n')\n",
    "            continue\n",
    "\n",
    "        sentences = split_sentences(paragraph)\n",
    "\n",
    "        output_sentences = []\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence or sentence == '...':\n",
    "                continue\n",
    "\n",
    "            tokens = tokenize_sentence(sentence)\n",
    "            tokens = clean_tokens(tokens)\n",
    "\n",
    "            if not tokens:\n",
    "                continue\n",
    "\n",
    "            sentence_count += 1\n",
    "            word_count += len(tokens)\n",
    "            char_count += sum(len(t) for t in tokens)\n",
    "            unique_tokens.update(tokens)\n",
    "\n",
    "            output_sentence = ' '.join(tokens)\n",
    "\n",
    "            if not output_sentence.endswith('.'):\n",
    "                output_sentence += '.'\n",
    "\n",
    "            output_sentences.append(output_sentence)\n",
    "\n",
    "        output_line = ' '.join(output_sentences)\n",
    "        outfile.write(output_line + '\\n')\n",
    "\n",
    "avg_sent_len = word_count / sentence_count\n",
    "avg_word_len = char_count / word_count\n",
    "ttr = len(unique_tokens) / word_count\n",
    "\n",
    "print(\"===== Corpus Statistics =====\")\n",
    "print(f\"Total Sentences: {sentence_count}\")\n",
    "print(f\"Total Words: {word_count}\")\n",
    "print(f\"Total Characters: {char_count}\")\n",
    "print(f\"Average Sentence Length (words): {avg_sent_len:.2f}\")\n",
    "print(f\"Average Word Length (chars): {avg_word_len:.2f}\")\n",
    "print(f\"Type/Token Ratio (TTR): {ttr:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
